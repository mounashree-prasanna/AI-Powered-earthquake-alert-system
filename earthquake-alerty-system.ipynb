{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FR_1qVxoEfPj"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import math\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, to_timestamp, year, month as pyspark_month, hour, dayofweek,\n",
        "    when, expr, lit, udf\n",
        ")\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, DoubleType, StringType,\n",
        "    TimestampType, IntegerType, ArrayType\n",
        ")\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Imputer\n",
        "from pyspark.ml.regression import RandomForestRegressionModel\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import to_timestamp, col\n",
        "\n",
        "from pyspark.sql.functions import when"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWcGGfzDEpLj"
      },
      "outputs": [],
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Earthquake Analysis\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOp4QvnTQ5xn"
      },
      "source": [
        "# **USGS DATA ML MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLUUqPhrZl0K"
      },
      "source": [
        "## **Data Preprocessing** - **Historical data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqhVuOg9NRUU"
      },
      "source": [
        "### Defining functions to get data from USGS API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3XmOKO4EsLq"
      },
      "outputs": [],
      "source": [
        "# Generating year-month combinations\n",
        "def get_year_month_pairs(start_year, end_year):\n",
        "    pairs = []\n",
        "    for year in range(start_year, end_year + 1):\n",
        "        for month in range(1, 13):\n",
        "            pairs.append((year, month))\n",
        "    return pairs\n",
        "\n",
        "# Building the API URL for the specific month\n",
        "def build_url(year, month):\n",
        "    start = f\"{year}-{month:02d}-01\"\n",
        "    if month == 12:\n",
        "        end = f\"{year+1}-01-01\"\n",
        "    else:\n",
        "        end = f\"{year}-{month+1:02d}-01\"\n",
        "    return (\n",
        "        f\"https://earthquake.usgs.gov/fdsnws/event/1/query?\"\n",
        "        f\"format=geojson&starttime={start}&endtime={end}&minmagnitude=1\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAfRh5wENfmm"
      },
      "source": [
        "### Fetches the data month wise from 2022 to 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIrnh_FBEwpQ",
        "outputId": "ed245c34-3532-41ec-cc40-5df9de18d541"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching 2022-01...\n",
            "Fetching 2022-02...\n",
            "Fetching 2022-03...\n",
            "Fetching 2022-04...\n",
            "Fetching 2022-05...\n",
            "Fetching 2022-06...\n",
            "Fetching 2022-07...\n",
            "Fetching 2022-08...\n",
            "Fetching 2022-09...\n",
            "Fetching 2022-10...\n",
            "Fetching 2022-11...\n",
            "Fetching 2022-12...\n",
            "Fetching 2023-01...\n",
            "Fetching 2023-02...\n",
            "Fetching 2023-03...\n",
            "Fetching 2023-04...\n",
            "Fetching 2023-05...\n",
            "Fetching 2023-06...\n",
            "Fetching 2023-07...\n",
            "Fetching 2023-08...\n",
            "Fetching 2023-09...\n",
            "Fetching 2023-10...\n",
            "Fetching 2023-11...\n",
            "Fetching 2023-12...\n",
            "Fetching 2024-01...\n",
            "Fetching 2024-02...\n",
            "Fetching 2024-03...\n",
            "Fetching 2024-04...\n",
            "Fetching 2024-05...\n",
            "Fetching 2024-06...\n",
            "Fetching 2024-07...\n",
            "Fetching 2024-08...\n",
            "Fetching 2024-09...\n",
            "Fetching 2024-10...\n",
            "Fetching 2024-11...\n",
            "Fetching 2024-12...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Collecting data into a Pandas DataFrame for easier type conversions\n",
        "records = []\n",
        "for (year, month) in get_year_month_pairs(2022, 2024):\n",
        "    url = build_url(year, month)\n",
        "    print(f\"Fetching {year}-{month:02d}...\")\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        for f in data['features']:\n",
        "            props = f['properties']\n",
        "            coords = f['geometry']['coordinates']\n",
        "\n",
        "            try:\n",
        "                time_ms = props['time']\n",
        "                time_val = datetime.fromtimestamp(time_ms / 1000) if time_ms else None\n",
        "\n",
        "                record = {\n",
        "                    'mag': float(props.get('mag')) if props.get('mag') is not None else None,\n",
        "                    'time': time_val,\n",
        "                    'tz': int(props.get('tz')) if props.get('tz') is not None else None,\n",
        "                    'felt': int(props.get('felt')) if props.get('felt') is not None else None,\n",
        "                    'cdi': float(props.get('cdi')) if props.get('cdi') is not None else None,\n",
        "                    'mmi': float(props.get('mmi')) if props.get('mmi') is not None else None,\n",
        "                    'alert': props.get('alert'),\n",
        "                    'status': props.get('status'),\n",
        "                    'tsunami': int(props.get('tsunami')) if props.get('tsunami') is not None else None,\n",
        "                    'sig': int(props.get('sig')) if props.get('sig') is not None else None,\n",
        "                    'nst': int(props.get('nst')) if props.get('nst') is not None else None,\n",
        "                    'dmin': float(props.get('dmin')) if props.get('dmin') is not None else None,\n",
        "                    'rms': float(props.get('rms')) if props.get('rms') is not None else None,\n",
        "                    'gap': float(props.get('gap')) if props.get('gap') is not None else None,\n",
        "                    'magType': props.get('magType'),\n",
        "                    'type': props.get('type'),\n",
        "                    'longitude': float(coords[0]) if coords[0] is not None else None,\n",
        "                    'latitude': float(coords[1]) if coords[1] is not None else None,\n",
        "                    'depth': float(coords[2]) if coords[2] is not None else None,\n",
        "                }\n",
        "                records.append(record)\n",
        "            except Exception as inner_err:\n",
        "                print(f\"Skipping a record due to error: {inner_err}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed for {year}-{month:02d}: {e}\")\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPZX--ekNnZs"
      },
      "source": [
        "### Defining the structure types for each feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E0zyz8_Exle"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Step 1: Create pandas DataFrame from records\n",
        "pandas_df = pd.DataFrame(records)\n",
        "pandas_df = pandas_df.where(pd.notnull(pandas_df), None)\n",
        "\n",
        "# Step 2: Replace NaNs with None for integer columns\n",
        "def safe_int(x):\n",
        "    try:\n",
        "        return int(x) if not pd.isna(x) and not math.isnan(x) else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def safe_float(x):\n",
        "    try:\n",
        "        return float(x) if not pd.isna(x) and not math.isnan(x) else None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Fix int-like columns\n",
        "int_columns = ['tsunami', 'sig']\n",
        "for col in int_columns:\n",
        "    pandas_df[col] = pandas_df[col].apply(safe_int)\n",
        "\n",
        "# Fix float columns\n",
        "float_columns = ['mag', 'dmin', 'rms', 'gap', 'longitude', 'latitude', 'depth']\n",
        "for col in float_columns:\n",
        "    pandas_df[col] = pandas_df[col].apply(safe_float)\n",
        "\n",
        "# Convert time to string (for later Spark to_timestamp conversion)\n",
        "if 'time' in pandas_df.columns:\n",
        "    pandas_df['time'] = pandas_df['time'].apply(lambda x: x.isoformat() if isinstance(x, datetime) else x if x is not None else None)\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"mag\", DoubleType(), True),\n",
        "    StructField(\"time\", StringType(), True),  # will cast to timestamp later\n",
        "    StructField(\"status\", StringType(), True),\n",
        "    StructField(\"tsunami\", IntegerType(), True),\n",
        "    StructField(\"sig\", IntegerType(), True),\n",
        "    StructField(\"dmin\", DoubleType(), True),\n",
        "    StructField(\"rms\", DoubleType(), True),\n",
        "    StructField(\"gap\", DoubleType(), True),\n",
        "    StructField(\"magType\", StringType(), True),\n",
        "    StructField(\"type\", StringType(), True),\n",
        "    StructField(\"longitude\", DoubleType(), True),\n",
        "    StructField(\"latitude\", DoubleType(), True),\n",
        "    StructField(\"depth\", DoubleType(), True),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S14MahZN3vI"
      },
      "source": [
        "### Creating spark dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvKaAtEuE5js",
        "outputId": "af7e7e7c-90a4-4e6a-b8b7-d080a906daac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- mag: double (nullable = true)\n",
            " |-- time: timestamp (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- tsunami: integer (nullable = true)\n",
            " |-- sig: integer (nullable = true)\n",
            " |-- dmin: double (nullable = true)\n",
            " |-- rms: double (nullable = true)\n",
            " |-- gap: double (nullable = true)\n",
            " |-- magType: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- depth: double (nullable = true)\n",
            "\n",
            "+----+--------------------+--------+-------+---+-------+------+------+-------+----------+------------+----------+-----+\n",
            "| mag|                time|  status|tsunami|sig|   dmin|   rms|   gap|magType|      type|   longitude|  latitude|depth|\n",
            "+----+--------------------+--------+-------+---+-------+------+------+-------+----------+------------+----------+-----+\n",
            "| 2.6|2022-01-31 23:54:...|reviewed|      0|104|  0.781|  0.33| 335.0|     ml|earthquake|    177.2704|   52.1081|13.14|\n",
            "| 1.3|2022-01-31 23:49:...|reviewed|      0| 26|  0.121|0.1533|123.71|     ml|earthquake|   -117.7476|    38.194|  9.2|\n",
            "|2.37|2022-01-31 23:45:...|reviewed|      0| 87| 0.0651|  0.14|  30.0|     ml|earthquake|-117.1991667|34.0088333|16.03|\n",
            "|1.92|2022-01-31 23:38:...|reviewed|      0| 57|0.07544|  0.16|  78.0|     md|earthquake|-121.1161667|36.6226667| 5.97|\n",
            "| 4.3|2022-01-31 23:08:...|reviewed|      0|284|  1.907|  0.65| 184.0|     mb|earthquake|    142.3778|   39.1693|48.14|\n",
            "+----+--------------------+--------+-------+---+-------+------+------+-------+----------+------------+----------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "records = pandas_df.to_dict(orient=\"records\")\n",
        "\n",
        "df = spark.createDataFrame(records, schema=schema)\n",
        "\n",
        "from pyspark.sql.functions import to_timestamp, col\n",
        "df = df.withColumn(\"time\", to_timestamp(col(\"time\")))\n",
        "\n",
        "df.printSchema()\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VilL94YGOCRs"
      },
      "source": [
        "### Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb_ckenhFFGl"
      },
      "outputs": [],
      "source": [
        "# Step 2: Extract time features\n",
        "df = df.withColumn(\"hour\", hour(col(\"time\"))) \\\n",
        "       .withColumn(\"dayofweek\", dayofweek(col(\"time\"))) \\\n",
        "       .withColumn(\"month\", pyspark_month(col(\"time\"))) \\\n",
        "\n",
        "# Step 3: Drop nulls for magnitude (our target variable)\n",
        "df = df.filter(col(\"mag\").isNotNull())\n",
        "\n",
        "# Step 4: Drop sparse columns (similar to pandas version)\n",
        "df = df.drop(\"cdi\", \"mmi\", \"felt\")\n",
        "\n",
        "# Step 5: Handle categorical variables\n",
        "categorical_cols = [c for c in [\"alert\", \"status\", \"magType\", \"type\"] if c in df.columns]\n",
        "\n",
        "# Update indexed_cols and encoder_output_cols based on available columns\n",
        "indexed_cols = [f\"{c}_indexed\" for c in categorical_cols]\n",
        "encoder_output_cols = [f\"{c}_encoded\" for c in categorical_cols]\n",
        "\n",
        "# Create stages for the pipeline\n",
        "stages = []\n",
        "\n",
        "# Indexing for categorical columns\n",
        "for categorical_col, indexed_col in zip(categorical_cols, indexed_cols):\n",
        "    indexer = StringIndexer(\n",
        "        inputCol=categorical_col,\n",
        "        outputCol=indexed_col,\n",
        "        handleInvalid=\"keep\"\n",
        "    )\n",
        "    stages.append(indexer)\n",
        "\n",
        "# One-hot encoding for indexed columns\n",
        "encoder = OneHotEncoder(\n",
        "    inputCols=indexed_cols,\n",
        "    outputCols=encoder_output_cols\n",
        ")\n",
        "stages.append(encoder)\n",
        "\n",
        "# Collect all numeric columns (excluding the target 'mag')\n",
        "numeric_cols = [col_name for col_name, data_type in df.dtypes\n",
        "               if data_type in (\"int\", \"double\") and col_name != \"mag\"]\n",
        "\n",
        "# Add the encoded categorical columns\n",
        "feature_cols = numeric_cols + encoder_output_cols\n",
        "\n",
        "imputer = Imputer(\n",
        "    inputCols=numeric_cols,\n",
        "    outputCols=numeric_cols\n",
        ").setStrategy(\"mean\")\n",
        "\n",
        "# Add the Imputer to the pipeline stages before the VectorAssembler\n",
        "stages.insert(0, imputer)\n",
        "\n",
        "# Vector assembler to combine all features\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols,\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"keep\"\n",
        ")\n",
        "stages.append(assembler)\n",
        "\n",
        "# Step 6: Create and apply the pipeline\n",
        "pipeline = Pipeline(stages=stages)\n",
        "pipeline_model = pipeline.fit(df)\n",
        "transformed_df_hist = pipeline_model.transform(df)\n",
        "\n",
        "# Save pipeline to use later on real-time data\n",
        "pipeline_model.write().overwrite().save(\"models/pipeline_model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIC5vMQpZ_pa"
      },
      "source": [
        "## **Data Preprocessing** - **Real time data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w5tCxc1ON4H"
      },
      "source": [
        "### Defining function to get real time data from USGS API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjZwp4PoZ-s5"
      },
      "outputs": [],
      "source": [
        "def build_url_last_hours(hours=2):\n",
        "    end = datetime.utcnow()\n",
        "    start = end - timedelta(hours=hours)\n",
        "    return (\n",
        "        f\"https://earthquake.usgs.gov/fdsnws/event/1/query?\"\n",
        "        f\"format=geojson&starttime={start.strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        f\"&endtime={end.strftime('%Y-%m-%dT%H:%M:%S')}&minmagnitude=3\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss-hCF5qOV99"
      },
      "source": [
        "### Fetches the data for last 2 hours"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX14E1OjaHDg",
        "outputId": "8a94a760-80dd-4427-fe61-384ad7538b8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching today's seismic data...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "records_real = []\n",
        "url = build_url_last_hours()\n",
        "print(\"Fetching today's seismic data...\")\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    data = response.json()\n",
        "\n",
        "    for f in data['features']:\n",
        "        props = f['properties']\n",
        "        coords = f['geometry']['coordinates']\n",
        "\n",
        "        try:\n",
        "            # Convert timestamp from milliseconds\n",
        "            time_ms = props['time']\n",
        "            time_val = datetime.fromtimestamp(time_ms / 1000) if time_ms else None\n",
        "\n",
        "            record = {\n",
        "                'mag': float(props.get('mag')) if props.get('mag') is not None else None,\n",
        "                'time': time_val,\n",
        "                'tz': int(props.get('tz')) if props.get('tz') is not None else None,\n",
        "                'felt': int(props.get('felt')) if props.get('felt') is not None else None,\n",
        "                'cdi': float(props.get('cdi')) if props.get('cdi') is not None else None,\n",
        "                'mmi': float(props.get('mmi')) if props.get('mmi') is not None else None,\n",
        "                'alert': props.get('alert'),\n",
        "                'status': props.get('status'),\n",
        "                'tsunami': int(props.get('tsunami')) if props.get('tsunami') is not None else None,\n",
        "                'sig': int(props.get('sig')) if props.get('sig') is not None else None,\n",
        "                'nst': int(props.get('nst')) if props.get('nst') is not None else None,\n",
        "                'dmin': float(props.get('dmin')) if props.get('dmin') is not None else None,\n",
        "                'rms': float(props.get('rms')) if props.get('rms') is not None else None,\n",
        "                'gap': float(props.get('gap')) if props.get('gap') is not None else None,\n",
        "                'magType': props.get('magType'),\n",
        "                'type': props.get('type'),\n",
        "                'longitude': float(coords[0]) if coords[0] is not None else None,\n",
        "                'latitude': float(coords[1]) if coords[1] is not None else None,\n",
        "                'depth': float(coords[2]) if coords[2] is not None else None,\n",
        "            }\n",
        "            records_real.append(record)\n",
        "        except Exception as inner_err:\n",
        "            print(f\"Skipping a record due to error: {inner_err}\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to fetch today's data: {e}\")\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, uuid\n",
        "\n",
        "stream_dir = \"stream_input\"\n",
        "os.makedirs(stream_dir, exist_ok=True)\n",
        "\n",
        "if records_real:\n",
        "    file_path = f\"{stream_dir}/quake_{uuid.uuid4().hex}.json\"\n",
        "    with open(file_path, \"w\") as f:\n",
        "        for record in records_real:\n",
        "            # Convert datetime to string\n",
        "            if isinstance(record.get(\"time\"), datetime):\n",
        "                record[\"time\"] = record[\"time\"].isoformat()\n",
        "            json.dump(record, f)\n",
        "            f.write(\"\\n\")\n",
        "    print(f\"Wrote {len(records_real)} records to {file_path}\")\n",
        "else:\n",
        "    print(\"No new records found in this interval.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZiBVSyHXzkE",
        "outputId": "599d912e-5cdf-45aa-8660-61d68be32cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote 1 records to stream_input/quake_84e8defcca4b456ba2960e6236fedd6b.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwwmlJuxOpAz"
      },
      "source": [
        "### Defining structure type for each feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHxmloqnaHAJ"
      },
      "outputs": [],
      "source": [
        "# Step 4: Define PySpark schema\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"mag\", DoubleType(), True),\n",
        "    StructField(\"time\", StringType(), True),\n",
        "    StructField(\"status\", StringType(), True),\n",
        "    StructField(\"tsunami\", IntegerType(), True),\n",
        "    StructField(\"sig\", IntegerType(), True),\n",
        "    StructField(\"dmin\", DoubleType(), True),\n",
        "    StructField(\"rms\", DoubleType(), True),\n",
        "    StructField(\"gap\", DoubleType(), True),\n",
        "    StructField(\"magType\", StringType(), True),\n",
        "    StructField(\"type\", StringType(), True),\n",
        "    StructField(\"longitude\", DoubleType(), True),\n",
        "    StructField(\"latitude\", DoubleType(), True),\n",
        "    StructField(\"depth\", DoubleType(), True),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7R8wuboO4Gk"
      },
      "source": [
        "### Creating spark dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWZ9eVEvaG8z",
        "outputId": "a814cf2c-a7f1-4331-b939-5150b0e80ae0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, to_timestamp\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "stream_df = spark.readStream \\\n",
        "    .schema(schema) \\\n",
        "    .json(\"stream_input\")\n",
        "\n",
        "# Fix timestamp\n",
        "stream_df = stream_df.withColumn(\"time\", to_timestamp(col(\"time\")))\n",
        "\n",
        "# Optional: filter, enrich, predict, etc.\n",
        "stream_df = stream_df.filter(col(\"mag\").isNotNull())\n",
        "\n",
        "# Output\n",
        "query = stream_df.writeStream \\\n",
        "    .format(\"console\") \\\n",
        "    .option(\"truncate\", False) \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination(timeout=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_timestamp, col\n",
        "stream_df = stream_df.withColumn(\"time\", to_timestamp(col(\"time\")))\n",
        "\n",
        "stream_df.printSchema()\n",
        "# stream_df.show(5)\n",
        "\n",
        "df_real = stream_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U_m5sAVX-HE",
        "outputId": "2c16d7cf-8c4d-4ac7-efab-5951f439b1db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- mag: double (nullable = true)\n",
            " |-- time: timestamp (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- tsunami: integer (nullable = true)\n",
            " |-- sig: integer (nullable = true)\n",
            " |-- dmin: double (nullable = true)\n",
            " |-- rms: double (nullable = true)\n",
            " |-- gap: double (nullable = true)\n",
            " |-- magType: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- longitude: double (nullable = true)\n",
            " |-- latitude: double (nullable = true)\n",
            " |-- depth: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc2VGGRkPByg"
      },
      "source": [
        "### Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7raUTx6aG44"
      },
      "outputs": [],
      "source": [
        "# Step 2: Extract time features\n",
        "df_real = df_real.withColumn(\"hour\", hour(col(\"time\"))) \\\n",
        "       .withColumn(\"dayofweek\", dayofweek(col(\"time\"))) \\\n",
        "       .withColumn(\"month\", pyspark_month(col(\"time\"))) \\\n",
        "\n",
        "# Step 3: Drop nulls for magnitude (our target variable)\n",
        "df_real = df_real.filter(col(\"mag\").isNotNull())\n",
        "\n",
        "# Step 4: Drop sparse columns (similar to pandas version)\n",
        "df_real = df_real.drop(\"cdi\", \"mmi\", \"felt\")\n",
        "\n",
        "# Check which categorical columns are actually present in the DataFrame\n",
        "categorical_cols = [c for c in [\"alert\", \"status\", \"magType\", \"type\"] if c in df_real.columns]\n",
        "\n",
        "# Update indexed_cols and encoder_output_cols based on available columns\n",
        "indexed_cols = [f\"{c}_indexed\" for c in categorical_cols]\n",
        "encoder_output_cols = [f\"{c}_encoded\" for c in categorical_cols]\n",
        "\n",
        "\n",
        "# Create stages for the pipeline\n",
        "stages = []\n",
        "\n",
        "# Indexing for categorical columns\n",
        "for categorical_col, indexed_col in zip(categorical_cols, indexed_cols):\n",
        "    indexer = StringIndexer(\n",
        "        inputCol=categorical_col,\n",
        "        outputCol=indexed_col,\n",
        "        handleInvalid=\"keep\"\n",
        "    )\n",
        "    stages.append(indexer)\n",
        "\n",
        "# One-hot encoding for indexed columns\n",
        "encoder = OneHotEncoder(\n",
        "    inputCols=indexed_cols,\n",
        "    outputCols=encoder_output_cols\n",
        ")\n",
        "stages.append(encoder)\n",
        "\n",
        "# Collect all numeric columns (excluding the target 'mag')\n",
        "numeric_cols = [col_name for col_name, data_type in df_real.dtypes\n",
        "               if data_type in (\"int\", \"double\") and col_name != \"mag\"]\n",
        "\n",
        "# Add the encoded categorical columns\n",
        "feature_cols = numeric_cols + encoder_output_cols\n",
        "\n",
        "imputer = Imputer(\n",
        "    inputCols=numeric_cols,\n",
        "    outputCols=numeric_cols\n",
        ").setStrategy(\"mean\")\n",
        "\n",
        "# Add the Imputer to the pipeline stages before the VectorAssembler\n",
        "stages.insert(0, imputer)\n",
        "\n",
        "# Vector assembler to combine all features\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=feature_cols,\n",
        "    outputCol=\"features\",\n",
        "    handleInvalid=\"keep\"\n",
        ")\n",
        "stages.append(assembler)\n",
        "\n",
        "\n",
        "from pyspark.ml import PipelineModel\n",
        "pipeline_model = PipelineModel.load(\"models/pipeline_model\")\n",
        "\n",
        "# Transform the real-time data using the trained pipeline\n",
        "transformed_df_real = pipeline_model.transform(df_real)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-JqlVs3PPiV"
      },
      "source": [
        "## **Training random forest model using historical data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsLPqzkzGOUW"
      },
      "outputs": [],
      "source": [
        "# Step 8: Train Random Forest model\n",
        "rf = RandomForestRegressor(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"mag\",\n",
        "    numTrees=100\n",
        ")\n",
        "rf_model = rf.fit(transformed_df_hist)\n",
        "\n",
        "rf_model.write().overwrite().save(\"models/rf_model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_train = rf_model.transform(transformed_df_hist)\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "evaluator = RegressionEvaluator(labelCol=\"mag\", predictionCol=\"prediction\")\n",
        "\n",
        "rmse_train = evaluator.setMetricName(\"rmse\").evaluate(predictions_train)\n",
        "mae_train = evaluator.setMetricName(\"mae\").evaluate(predictions_train)\n",
        "r2_train = evaluator.setMetricName(\"r2\").evaluate(predictions_train)\n",
        "\n",
        "print(f\"Training RMSE: {rmse_train}\")\n",
        "print(f\"Training MAE: {mae_train}\")\n",
        "print(f\"Training R²: {r2_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-pl-65fNC9x",
        "outputId": "da227883-c48f-4e40-8c15-425621d28f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RMSE: 0.16507720437170917\n",
            "Training MAE: 0.11903806494432777\n",
            "Training R²: 0.9784576938348237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IGMj-VYP4XP"
      },
      "source": [
        "## **Predicting using real time data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e32vGKdxcSXG"
      },
      "outputs": [],
      "source": [
        "# Step 9: Make predictions\n",
        "rf_model = RandomForestRegressionModel.load(\"models/rf_model\")\n",
        "\n",
        "# Predict on real-time transformed data\n",
        "pred_results = rf_model.transform(transformed_df_real)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlClEJ2PGQZS"
      },
      "outputs": [],
      "source": [
        "pred_results = pred_results.select(\n",
        "    \"*\",\n",
        "    col(\"mag\").alias(\"actual_mag\"),\n",
        "    col(\"prediction\").alias(\"predicted_mag\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rrtaJZsQGuR"
      },
      "source": [
        "### Saving the predictions to predicted_earthquakes.csv file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SieKa4nSH7tm",
        "outputId": "d964b81c-3879-4324-f2ef-3c9c09ad4f0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "clean_df = pred_results.drop(\"status_encoded\", \"features\", \"magType_encoded\", \"type_encoded\")\n",
        "\n",
        "query = clean_df.writeStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .option(\"path\", \"predicted_earthquakes\") \\\n",
        "    .option(\"checkpointLocation\", \"predicted_earthquakes_checkpoint\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination(timeout=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW9hU0GXQTpC"
      },
      "source": [
        "## **Defining magnitude buckets to create different impact levels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51URf1yrK5fJ"
      },
      "outputs": [],
      "source": [
        "  # Make sure to import col\n",
        "\n",
        "bucketed = pred_results.withColumn(\n",
        "    \"impact_level\",\n",
        "    when(col(\"predicted_mag\") < 4.0, \"Minor\")\n",
        "    .when(col(\"predicted_mag\") < 5.0, \"Light\")\n",
        "    .when(col(\"predicted_mag\") < 6.0, \"Moderate\")\n",
        "    .when(col(\"predicted_mag\") < 7.0, \"Strong\")\n",
        "    .when(col(\"predicted_mag\") < 8.0, \"Major\")\n",
        "    .otherwise(\"Great\")\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmHAx_ghQhkX"
      },
      "source": [
        "### Storing the results to predicted_earthquakes_with_impact.csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbBZU6v_-Mo8",
        "outputId": "8f4f8b9d-1cb2-4b49-a8dd-f5a1b434d92a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "bucketed = bucketed.drop(\"status_encoded\", \"magType_encoded\", \"type_encoded\", \"features\") # Drop the 'features' column\n",
        "\n",
        "query_bucket = bucketed.writeStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .option(\"path\", \"predicted_earthquakes_with_impact\") \\\n",
        "    .option(\"checkpointLocation\", \"predicted_earthquakes_with_impact_checkpoint\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .start()\n",
        "\n",
        "query_bucket.awaitTermination(timeout=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08VW198hMo3_"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "for file in os.listdir(\"predicted_earthquakes_with_impact\"):\n",
        "    if file.startswith(\"part-\") and file.endswith(\".csv\"):\n",
        "        shutil.move(f\"predicted_earthquakes_with_impact/{file}\", \"predicted_earthquakes_with_impact.csv\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8NJRpvTpQDLV"
      },
      "outputs": [],
      "source": [
        "bucket_counts = bucketed.groupBy(\"impact_level\").count().orderBy(\"impact_level\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlFtu9fv5D-u"
      },
      "source": [
        " # **SENTIMENT ANALYSIS ON NEWS DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M61u4u-55Ld1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "474fd893-5db5-412d-97f3-d459cfc4089b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final file saved to: 'earthquake_news_enriched_output/'\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf, col, when, concat_ws\n",
        "from pyspark.sql.types import StringType, ArrayType, FloatType\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"EarthquakeImpactAnalysis\").getOrCreate()\n",
        "\n",
        "# Download required VADER lexicon\n",
        "nltk.download(\"vader_lexicon\")\n",
        "\n",
        "# Load CSV file\n",
        "df = spark.read.option(\"header\", True).csv(\"news_earthquake.csv\")\n",
        "\n",
        "# Fill missing descriptions\n",
        "df = df.withColumn(\"Description\", when(col(\"Description\").isNull(), \"\").otherwise(col(\"Description\")))\n",
        "\n",
        "# ✅ Fixed UDF: Get compound sentiment score\n",
        "def get_compound_score(text):\n",
        "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    return float(sid.polarity_scores(text)['compound'])\n",
        "\n",
        "sentiment_udf = udf(get_compound_score, FloatType())\n",
        "df = df.withColumn(\"Compound\", sentiment_udf(col(\"Description\")))\n",
        "\n",
        "# UDF: Get sentiment label\n",
        "def get_sentiment_label(compound):\n",
        "    if compound >= 0.05:\n",
        "        return \"Positive\"\n",
        "    elif compound <= -0.05:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "sentiment_label_udf = udf(get_sentiment_label, StringType())\n",
        "df = df.withColumn(\"Sentiment_Label\", sentiment_label_udf(col(\"Compound\")))\n",
        "\n",
        "# Impact keywords list\n",
        "impact_keywords = [\n",
        "    \"death\", \"deaths\", \"dead\", \"fatalities\", \"killed\", \"casualties\",\n",
        "    \"injured\", \"hurt\", \"wounded\", \"critical condition\", \"trauma\",\n",
        "    \"missing\", \"unaccounted\", \"trapped\", \"buried\", \"suffocated\",\n",
        "    \"collapsed\", \"destroyed\", \"damaged\", \"ruined\", \"wreckage\",\n",
        "    \"cracks\", \"debris\", \"rubble\", \"fractured\", \"sinkhole\", \"tilted\",\n",
        "    \"building collapse\", \"infrastructure failure\", \"landslide\", \"roadblock\",\n",
        "    \"rescue\", \"rescued\", \"search operations\", \"emergency\", \"firefighters\",\n",
        "    \"evacuated\", \"evacuation\", \"relocated\", \"displacement\", \"cleared\",\n",
        "    \"first responders\", \"military assistance\", \"national guard\",\n",
        "    \"earthquake\", \"aftershocks\", \"tsunami\", \"tremors\", \"shockwave\", \"seismic wave\",\n",
        "    \"ground shaking\", \"earth ruptured\", \"surface crack\", \"liquefaction\",\n",
        "    \"aid\", \"relief\", \"donations\", \"shelter\", \"tents\", \"food supply\",\n",
        "    \"water shortage\", \"NGO\", \"Red Cross\", \"emergency services\", \"crisis response\",\n",
        "    \"humanitarian\",\n",
        "    \"panic\", \"fear\", \"chaos\", \"devastation\", \"mental health\", \"helpline\",\n",
        "    \"disruption\", \"unrest\", \"protest\", \"looting\", \"violence\"\n",
        "]\n",
        "\n",
        "# UDF: Normalize suffixes\n",
        "def normalize(word):\n",
        "    for suffix in [\"ing\", \"ed\", \"es\", \"s\", \"ly\", \"ment\"]:\n",
        "        if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
        "            return word[:-len(suffix)]\n",
        "    return word\n",
        "\n",
        "# UDF: Extract impact keywords\n",
        "def extract_keywords(text):\n",
        "    text = text.lower()\n",
        "    found = []\n",
        "    for kw in impact_keywords:\n",
        "        if normalize(kw.lower()) in text:\n",
        "            found.append(kw)\n",
        "    return list(set(found))\n",
        "\n",
        "keyword_udf = udf(extract_keywords, ArrayType(StringType()))\n",
        "df = df.withColumn(\"Impact_Keywords\", keyword_udf(col(\"Description\")))\n",
        "\n",
        "# UDF: Extract magnitude\n",
        "def extract_magnitude(text):\n",
        "    text = text.lower()\n",
        "    patterns = [\n",
        "        r'magnitude[\\s:]*([\\d]{1,2}(\\.\\d{1,2})?)',\n",
        "        r'([\\d]{1,2}(\\.\\d{1,2})?)\\s*magnitude',\n",
        "        r'mw[\\s:]*([\\d]{1,2}(\\.\\d{1,2})?)',\n",
        "        r'([\\d]{1,2}(\\.\\d{1,2})?)-magnitude'\n",
        "    ]\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, text)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "    return None\n",
        "\n",
        "magnitude_udf = udf(extract_magnitude, StringType())\n",
        "df = df.withColumn(\"Extracted_Magnitude\", magnitude_udf(col(\"Description\")))\n",
        "\n",
        "# UDF: Assign impact severity bucket\n",
        "def assign_bucket(compound, magnitude, keywords):\n",
        "    try:\n",
        "        magnitude = float(magnitude) if magnitude else None\n",
        "    except:\n",
        "        magnitude = None\n",
        "\n",
        "    keyword_count = len(keywords) if keywords else 0\n",
        "\n",
        "    if magnitude:\n",
        "        if magnitude >= 7.0 and compound <= -0.3 and keyword_count >= 4:\n",
        "            return \"Major\"\n",
        "        elif 6.0 <= magnitude < 7.0 and compound <= -0.2 and keyword_count >= 3:\n",
        "            return \"Strong\"\n",
        "        elif 5.0 <= magnitude < 6.0 and keyword_count >= 2:\n",
        "            return \"Moderate\"\n",
        "        elif 4.0 <= magnitude < 5.0 and keyword_count >= 1:\n",
        "            return \"Light\"\n",
        "        else:\n",
        "            return \"Minor\"\n",
        "\n",
        "    if compound <= -0.4 and keyword_count >= 4:\n",
        "        return \"Major\"\n",
        "    elif compound <= -0.3 and keyword_count >= 2:\n",
        "        return \"Strong\"\n",
        "    elif compound <= -0.2 and keyword_count >= 2:\n",
        "        return \"Moderate\"\n",
        "    elif keyword_count >= 1:\n",
        "        return \"Light\"\n",
        "    else:\n",
        "        return \"Minor\"\n",
        "\n",
        "bucket_udf = udf(assign_bucket, StringType())\n",
        "df = df.withColumn(\"Impact_Severity_Bucket\", bucket_udf(col(\"Compound\"), col(\"Extracted_Magnitude\"), col(\"Impact_Keywords\")))\n",
        "\n",
        "# Convert Impact_Keywords array to string (CSV-safe)\n",
        "df = df.withColumn(\"Impact_Keywords_Str\", concat_ws(\", \", col(\"Impact_Keywords\")))\n",
        "df_final = df.drop(\"Impact_Keywords\")\n",
        "\n",
        "\n",
        "df_final.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"earthquake_news_enriched_output\")\n",
        "\n",
        "print(\"Final file saved to: 'earthquake_news_enriched_output/'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "for file in os.listdir(\"earthquake_news_enriched_output\"):\n",
        "    if file.startswith(\"part-\") and file.endswith(\".csv\"):\n",
        "        shutil.move(f\"earthquake_news_enriched_output/{file}\", \"earthquake_news_enriched.csv\")\n",
        "        break\n",
        "\n",
        "print(\"Saved as: earthquake_news_enriched.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJX_ngiNaylw",
        "outputId": "d9441350-2849-4e7d-dbbe-a84fd9c2f1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved as: earthquake_news_enriched.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDIrKLOgMQyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27f66e21-3bd5-4e3b-ef06-fc448efb2064"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+---------------------------------------------------------------------------------------------+--------------------------------------------------+--------------+\n",
            "|Impact_Severity_Bucket|Top_Keywords                                                                                 |Sentiment_Distribution                            |total_articles|\n",
            "+----------------------+---------------------------------------------------------------------------------------------+--------------------------------------------------+--------------+\n",
            "|Minor                 |[earthquake, aid, killed, tsunami, damaged, panic, tremors, injured, casualties, dead]       |[Positive: 0.152, Neutral: 0.156, Negative: 0.692]|653           |\n",
            "|Light                 |[earthquake, aid, damaged, tremors, rescue, rescued, injured, rubble, tsunami, casualties]   |[Neutral: 0.272, Negative: 0.401, Positive: 0.327]|624           |\n",
            "|Strong                |[earthquake, damaged, injured, aid, killed, tsunami, casualties, tremors, collapsed, deaths] |[Negative: 1.0]                                   |386           |\n",
            "|Major                 |[earthquake, damaged, killed, injured, aid, death, deaths, rescue, rescued, casualties]      |[Negative: 1.0]                                   |217           |\n",
            "|Moderate              |[earthquake, damaged, aid, tremors, injured, casualties, aftershocks, deaths, death, tsunami]|[Positive: 0.016, Negative: 0.897, Neutral: 0.087]|126           |\n",
            "+----------------------+---------------------------------------------------------------------------------------------+--------------------------------------------------+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lower, split, explode, trim, count, collect_list, expr\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Start Spark session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Step 2: Load CSV with flexible options to handle bad rows\n",
        "df2 = spark.read.option(\"header\", \"true\") \\\n",
        "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
        "    .csv(\"earthquake_news_enriched.csv\")\n",
        "\n",
        "# Step 3: Filter rows with non-null Impact_Severity_Bucket and Impact_Keywords_Str\n",
        "df2_cleaned = df2.filter(\n",
        "    col(\"Impact_Severity_Bucket\").isNotNull() &\n",
        "    col(\"Impact_Keywords_Str\").isNotNull()\n",
        ")\n",
        "\n",
        "# Step 4: Normalize and explode keywords\n",
        "df2_keywords = df2_cleaned.withColumn(\n",
        "    \"keyword\", explode(split(lower(col(\"Impact_Keywords_Str\")), \",\"))\n",
        ").withColumn(\"keyword\", trim(col(\"keyword\")))\n",
        "\n",
        "# Step 5: Group by severity and keyword, then count\n",
        "keyword_counts = df2_keywords.groupBy(\"Impact_Severity_Bucket\", \"keyword\") \\\n",
        "    .agg(count(\"*\").alias(\"keyword_count\"))\n",
        "\n",
        "# Step 6: Get Top 10 keywords per Impact_Severity_Bucket\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "\n",
        "window_spec = Window.partitionBy(\"Impact_Severity_Bucket\").orderBy(col(\"keyword_count\").desc())\n",
        "\n",
        "top_keywords = keyword_counts.withColumn(\"rank\", row_number().over(window_spec)) \\\n",
        "    .filter(col(\"rank\") <= 10) \\\n",
        "    .groupBy(\"Impact_Severity_Bucket\") \\\n",
        "    .agg(collect_list(\"keyword\").alias(\"Top_Keywords\"))\n",
        "\n",
        "# Step 7: Sentiment distribution per Impact_Severity_Bucket\n",
        "sentiment_dist = df2_cleaned.groupBy(\"Impact_Severity_Bucket\", \"Sentiment_Label\") \\\n",
        "    .agg(count(\"*\").alias(\"count\"))\n",
        "\n",
        "# Total articles per severity for calculating sentiment %\n",
        "total_per_bucket = df2_cleaned.groupBy(\"Impact_Severity_Bucket\") \\\n",
        "    .agg(count(\"*\").alias(\"total_articles\"))\n",
        "\n",
        "sentiment_dist_pct = sentiment_dist.join(total_per_bucket, on=\"Impact_Severity_Bucket\") \\\n",
        "    .withColumn(\"sentiment_pct\", expr(\"round(count / total_articles, 3)\")) \\\n",
        "    .groupBy(\"Impact_Severity_Bucket\") \\\n",
        "    .agg(\n",
        "        collect_list(\n",
        "            expr(\"concat(Sentiment_Label, ': ', sentiment_pct)\")\n",
        "        ).alias(\"Sentiment_Distribution\")\n",
        "    )\n",
        "\n",
        "# Step 8: Join keywords and sentiment into final reference table\n",
        "impact_summary_top10_df = top_keywords.join(sentiment_dist_pct, on=\"Impact_Severity_Bucket\") \\\n",
        "    .join(total_per_bucket, on=\"Impact_Severity_Bucket\")\n",
        "\n",
        "# Step 9: Show result\n",
        "impact_summary_top10_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat_ws, col\n",
        "\n",
        "impact_summary_top10_flat = impact_summary_top10_df \\\n",
        "    .withColumn(\"Top_Keywords\", concat_ws(\", \", col(\"Top_Keywords\"))) \\\n",
        "    .withColumn(\"Sentiment_Distribution\", concat_ws(\", \", col(\"Sentiment_Distribution\")))\n",
        "\n",
        "impact_summary_top10_flat.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"impact_summary_top10_csv\")\n"
      ],
      "metadata": {
        "id": "PbkpYHk1Qlpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "\n",
        "for f in os.listdir(\"impact_summary_top10_csv\"):\n",
        "    if f.startswith(\"part-\") and f.endswith(\".csv\"):\n",
        "        shutil.move(os.path.join(\"impact_summary_top10_csv\", f), \"impact_summary_top10.csv\")\n",
        "        break\n",
        "\n",
        "shutil.rmtree(\"impact_summary_top10_csv\")\n",
        "print(\"Saved as impact_summary_top10.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stanq0j_Q4bs",
        "outputId": "a1a02d8a-8930-4044-d2cc-ba738e4c47d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved as impact_summary_top10.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjHJEosogmpe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61b62a6-3eb1-4645-9981-10e5d70415d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " EARTHQUAKE ALERT MESSAGE:\n",
            "\n",
            " Earthquake Alert: **Moderate Severity** (Predicted Magnitude: 5.469764185157726)\n",
            " Location: Latitude -7.1234, Longitude 129.138 (Maluku, Indonesia)\n",
            " Based on 126 historical news articles, such earthquakes were often associated with:\n",
            " Common impacts: earthquake, damaged, aid, tremors, injured, casualties, aftershocks, deaths, death,\n",
            "tsunami\n",
            " Sentiment response: Neutral: 8.7%, Negative: 89.7%, Positive: 1.6%\n",
            "Around 89.7% of the sentiment in related articles showed negative emotion, indicating serious\n",
            "concern in past similar events. Events of moderate severity often involve earthquake, damaged, or\n",
            "aid. Monitor updates and ensure surroundings are safe.\n",
            "Alert email sent successfully.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, desc\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "import textwrap\n",
        "import smtplib\n",
        "from email.message import EmailMessage\n",
        "\n",
        "# Step 0: Start Spark session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Step 1: Load real-time seismic predictions\n",
        "df1_spark = spark.read.option(\"header\", True).csv(\"predicted_earthquakes_with_impact.csv\")\n",
        "\n",
        "# Step 2: Get latest event by time\n",
        "latest_event_row = df1_spark.orderBy(desc(\"time\")).limit(1).collect()[0]\n",
        "\n",
        "# Step 3: Extract values\n",
        "predicted_mag = float(latest_event_row[\"predicted_mag\"])\n",
        "impact_level = latest_event_row[\"impact_level\"]\n",
        "latitude = float(latest_event_row[\"latitude\"])\n",
        "longitude = float(latest_event_row[\"longitude\"])\n",
        "\n",
        "# Step 4: Convert reference table to dictionary\n",
        "reference_pd = impact_summary_top10_df.toPandas()\n",
        "reference_dict = reference_pd.set_index(\"Impact_Severity_Bucket\").T.to_dict()\n",
        "\n",
        "# Step 5: Reverse geocoding with fallback\n",
        "def reverse_geocode(lat, lon):\n",
        "    geolocator = Nominatim(user_agent=\"earthquake-alert-system\")\n",
        "    try:\n",
        "        location = geolocator.reverse((lat, lon), timeout=10)\n",
        "        if location and location.address:\n",
        "            return location.address\n",
        "        else:\n",
        "            return \"Open ocean region (no nearby land)\"\n",
        "    except GeocoderTimedOut:\n",
        "        return \"Geocoding timed out\"\n",
        "\n",
        "location_name = reverse_geocode(latitude, longitude)\n",
        "\n",
        "# Step 6: Generate alert message\n",
        "def generate_earthquake_alert(impact_level, predicted_mag, latitude, longitude, location_name, ref_dict):\n",
        "    if impact_level not in ref_dict:\n",
        "        return f\"No reference data available for impact level: {impact_level}\"\n",
        "\n",
        "    row = ref_dict[impact_level]\n",
        "    keywords = row[\"Top_Keywords\"]\n",
        "    sentiments = row[\"Sentiment_Distribution\"]\n",
        "    article_count = row[\"total_articles\"]\n",
        "\n",
        "    # Sentiment parsing\n",
        "    if isinstance(sentiments, dict):\n",
        "        sentiment_scores = sentiments\n",
        "    else:\n",
        "        sentiment_scores = {s.split(\":\")[0]: float(s.split(\":\")[1]) for s in sentiments}\n",
        "\n",
        "    neutral = sentiment_scores.get(\"Neutral\", 0)\n",
        "    negative = sentiment_scores.get(\"Negative\", 0)\n",
        "    positive = sentiment_scores.get(\"Positive\", 0)\n",
        "\n",
        "    neutral_pct = round(neutral * 100, 1)\n",
        "    negative_pct = round(negative * 100, 1)\n",
        "    positive_pct = round(positive * 100, 1)\n",
        "\n",
        "    sentiment_str = f\"Neutral: {neutral_pct}%, Negative: {negative_pct}%, Positive: {positive_pct}%\"\n",
        "    keyword_str = \", \".join(keywords[:10])\n",
        "    key1, key2, key3 = keywords[0], keywords[1], keywords[2]\n",
        "\n",
        "    if negative_pct > 60:\n",
        "        sentiment_msg = f\"Around {negative_pct}% of the sentiment in related articles showed negative emotion, indicating serious concern in past similar events.\"\n",
        "    elif negative_pct > 40:\n",
        "        sentiment_msg = f\"Approximately {negative_pct}% of articles reflected negative sentiment — a moderate level of public concern.\"\n",
        "    elif negative_pct > 25:\n",
        "        sentiment_msg = f\"Only about {negative_pct}% of the historical sentiment was negative. This is generally not a cause for major alarm.\"\n",
        "    else:\n",
        "        sentiment_msg = f\"Less than {negative_pct}% of the sentiment was negative, suggesting that public response was calm in similar cases.\"\n",
        "\n",
        "    # Ocean detection\n",
        "    location_lower = location_name.lower()\n",
        "    ocean_keywords = [\"ocean\", \"sea\", \"trench\", \"open water\", \"open ocean\", \"gulf\"]\n",
        "    is_oceanic = any(word in location_lower for word in ocean_keywords)\n",
        "\n",
        "    # Advisory logic\n",
        "    if is_oceanic:\n",
        "        advisory = (\n",
        "            f\"This earthquake occurred in the ocean region ({location_name}). \"\n",
        "            f\"While similar earthquakes on land have shown sentiment patterns like: {sentiment_msg.lower()} \"\n",
        "            f\"and often involved impacts like {key1}, {key2}, and {key3}, there is currently no land nearby. \"\n",
        "            f\"As a result, this event is not considered a cause for public concern.\"\n",
        "        )\n",
        "    else:\n",
        "        level = impact_level.lower()\n",
        "        if level == \"minor\":\n",
        "            advisory = (\n",
        "                f\"{sentiment_msg} Minor tremors like these were typically linked to terms like {key1} and {key2}. \"\n",
        "                f\"No action is required — just stay informed.\"\n",
        "            )\n",
        "        elif level == \"light\":\n",
        "            advisory = (\n",
        "                f\"{sentiment_msg} Light earthquakes have historically been associated with {key1} and {key2}. \"\n",
        "                f\"No structural damage expected — stay aware.\"\n",
        "            )\n",
        "        elif level == \"moderate\":\n",
        "            advisory = (\n",
        "                f\"{sentiment_msg} Events of moderate severity often involve {key1}, {key2}, or {key3}. \"\n",
        "                f\"Monitor updates and ensure surroundings are safe.\"\n",
        "            )\n",
        "        elif level == \"strong\":\n",
        "            advisory = (\n",
        "                f\"{sentiment_msg} Strong earthquakes like this have previously included impacts such as {key1}, {key2}, and {key3}. \"\n",
        "                f\"Take shelter and prepare for aftershocks.\"\n",
        "            )\n",
        "        elif level == \"major\":\n",
        "            advisory = (\n",
        "                f\"{sentiment_msg} Major earthquakes are commonly associated with {key1}, {key2}, and {key3}. \"\n",
        "                f\"Follow emergency procedures and official alerts without delay.\"\n",
        "            )\n",
        "        else:\n",
        "            advisory = f\"{sentiment_msg} Stay alert and follow emergency protocols if applicable.\"\n",
        "\n",
        "    # Final alert message\n",
        "    alert_msg = (\n",
        "        f\" Earthquake Alert: **{impact_level} Severity** (Predicted Magnitude: {predicted_mag})\\n\"\n",
        "        f\" Location: Latitude {latitude}, Longitude {longitude} ({location_name})\\n\"\n",
        "        f\" Based on {article_count} historical news articles, such earthquakes were often associated with:\\n\"\n",
        "        f\" Common impacts: {keyword_str}\\n\"\n",
        "        f\" Sentiment response: {sentiment_str}\\n\"\n",
        "        f\"{advisory}\"\n",
        "    )\n",
        "    return alert_msg\n",
        "\n",
        "alert_message = generate_earthquake_alert(\n",
        "    impact_level, predicted_mag, latitude, longitude, location_name, reference_dict\n",
        ")\n",
        "\n",
        "# Step 7: Print message in terminal with wrapping\n",
        "print(\"\\n EARTHQUAKE ALERT MESSAGE:\\n\")\n",
        "for paragraph in alert_message.split(\"\\n\"):\n",
        "    print(textwrap.fill(paragraph, width=100))\n",
        "\n",
        "# Step 8: Email utility\n",
        "def send_email_alert(subject, body, to_email, from_email, app_password):\n",
        "    msg = EmailMessage()\n",
        "    msg['Subject'] = subject\n",
        "    msg['From'] = from_email\n",
        "    msg['To'] = to_email\n",
        "    msg.set_content(body)\n",
        "\n",
        "    try:\n",
        "        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as smtp:\n",
        "            smtp.login(from_email, app_password)\n",
        "            smtp.send_message(msg)\n",
        "        print(\"Alert email sent successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to send email: {e}\")\n",
        "\n",
        "\n",
        "wrapped_alert = \"\\n\".join([textwrap.fill(p, width=100) for p in alert_message.split(\"\\n\")])\n",
        "\n",
        "\n",
        "subject = f\"Earthquake Alert: {impact_level} Severity\"\n",
        "to_email = \"guannan.liu@sjsu.edu\"\n",
        "from_email = \"chetana.muralidharan.24@gmail.com\"\n",
        "app_password = \"gsdokgwxycdrfsfd\"\n",
        "\n",
        "send_email_alert(subject, wrapped_alert, to_email, from_email, app_password)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
